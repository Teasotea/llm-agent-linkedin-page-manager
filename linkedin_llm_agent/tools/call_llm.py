from typing import Optional

from openai import OpenAI, NotGiven, NOT_GIVEN

#from linkedin_llm_agent.setup import model
model = "gpt-4o-mini"
client = OpenAI()

chat_gpt_client = OpenAI()

def call_llm(
        prompt: str,
        system_promt: str = "You are a helpful agent.",
        model_name: str = model,
        max_tokens: Optional[int] | NotGiven = NOT_GIVEN
) -> str:
    """
    Calls a language model (LLM) with a given prompt and configuration.

    Args:
        prompt (str): The prompt to provide to the LLM.
        system_promt (str): The system-level prompt for context. Defaults to "You are a helpful agent."
        model_name (str): The name of the LLM model to use. Defaults to the value of `model`.
        max_tokens (Optional[int] | NotGiven): The maximum number of tokens for the response. Defaults to `NOT_GIVEN`.

    Returns:
        str: The response generated by the LLM.
    """
    response = chat_gpt_client.chat.completions.create(
        model=model_name,
        messages=[
            { "role": "system", "content": system_promt },
            {
                "role": "user",
                "content": prompt,
            }
        ],
        n=1, # generate only one serponce
        temperature=0.0, # to be deterministic (for testing)
        max_tokens=max_tokens,  # to make responces shorter
        seed=42, # to be deterministic (for testing)
    )
    return response.choices[0].message.content